{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate OpenDP Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The majority of the library lies behind \"contrib\", a feature-gated, opt-in flag. \n",
    "Each component in the library must pass a vetting process before it is moved out of \"contrib\".\n",
    "I am opting into \"contrib\" because the analysis in this notebook will need to use features that have not passed the vetting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendp.mod import enable_features\n",
    "enable_features('contrib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations Intuition\n",
    "\n",
    "The bulk of the library interface consists of constructors. One such example is a constructor for a clamp transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 3, 5, 10, 10]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendp.trans import make_clamp\n",
    "clamper = make_clamp(bounds=(0, 10))\n",
    "\n",
    "clamper([-1, 0, 3, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamper.check(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use chaining to combine multiple transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 10, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendp.trans import make_cast_default\n",
    "\n",
    "caster = make_cast_default(TIA=str, TOA=int)\n",
    "\n",
    "preprocessor = caster >> clamper\n",
    "preprocessor([\"1\", \"2\", \"3\", \"20\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.check(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input metric and output metric are implicitly `SymmetricDistance`.\n",
    "\n",
    "Lets start a more fleshed out analysis of the PUMS dataset.\n",
    "\n",
    "Let's examine how we can process csv data with Transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# establish public information\n",
    "col_names = [\"state\", \"puma\", \"sex\", \"age\", \"educ\", \"income\", \"latino\", \"black\", \"asian\", \"married\"]\n",
    "# the greatest number of records that any one individual can influence in the dataset\n",
    "max_influence = 1\n",
    "# we can also reasonably intuit that age and income will be numeric,\n",
    "#     as well as bounds for them, without looking at the data\n",
    "age_bounds = (0, 100)\n",
    "income_bounds = (0, 500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833320,6,64900,0,45,6,6000,0,0,0,1\n",
      "875563,6,66117,1,41,8,13000,1,0,0,1\n",
      "431101,6,62802,0,63,14,17810,0,0,0,1\n",
      "941157,6,66123,1,71,15,3600,0,0,1,1\n",
      "35464,6,60600,0,44,5,10000,0,0,0,0\n",
      "451867,6,63301,1,49,1,0,0,0,1,0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = os.path.join('.', 'PUMSExtract1000.csv')\n",
    "\n",
    "with open(data_path) as input_file:\n",
    "    data = input_file.read()\n",
    "\n",
    "print('\\n'.join(data.split('\\n')[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see from the first few rows, it is intentional that there are no column names in the data.\n",
    "If your data has column names, you will want to strip them out before passing data into your function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to pull a few constructors from the [Dataframes section in the user guide](https://docs.opendp.org/en/stable/user/transformation-constructors.html#dataframe).\n",
    "\n",
    "We start with `make_split_dataframe` to parse one large string containing all the csv data into a dataframe.\n",
    "`make_split_dataframe` expects us to pass column names, which we can grab out of the public information.\n",
    "`make_select_column` will then index into the dataframe to pull out a column where the elements have a given type `TOA`.\n",
    "The `TOA` argument won't cast for you; the casting comes later!\n",
    "\n",
    "Now if you run the transformation on the data, you will get a list of incomes as strings.\n",
    "I've limited the output to just the first few income values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from opendp.trans import make_split_dataframe, make_select_column\n",
    "income_preprocessor = (\n",
    "    # Convert data into a dataframe where columns are of type Vec<str>\n",
    "    make_split_dataframe(separator=\",\", col_names=col_names) >>\n",
    "    # Selects a column of df, Vec<str>\n",
    "    make_select_column(key=\"income\", TOA=str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['6', '8', '14', '15', '5', '1']\n"
     ]
    }
   ],
   "source": [
    "transformed = income_preprocessor(data)\n",
    "print(type(transformed))\n",
    "print(transformed[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting\n",
    "Income doesn't make sense as a string for our purposes,\n",
    "so we can just extend the previous preprocessor to also cast and impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 14, 15, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "from opendp.trans import make_cast, make_impute_constant\n",
    "\n",
    "# make a transformation that casts from a vector of strings to a vector of ints\n",
    "cast_str_int = (\n",
    "    # Cast Vec<str> to Vec<Option<int>>\n",
    "    make_cast(TIA=str, TOA=int) >>\n",
    "    # Replace any elements that failed to parse with 0, emitting a Vec<int>\n",
    "    make_impute_constant(0)\n",
    ")\n",
    "\n",
    "# replace the previous preprocessor: extend it with the caster\n",
    "income_preprocessor = income_preprocessor >> cast_str_int\n",
    "print(income_preprocessor(data)[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've successfully transformed CSV data into an integer vector from the income column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Structure\n",
    "The general approach is that we can build up lengthy computation chains out of isolated transformations.\n",
    "Each constituent transformation represents an isolated unit of computation with provable stability properties.\n",
    "\n",
    "The following snip shows the definition of a Transformation in the core rust library.\n",
    "```rust\n",
    "pub struct Transformation<DI: Domain, DO: Domain, MI: Metric, MO: Metric> {\n",
    "    pub input_domain: DI,\n",
    "    pub output_domain: DO,\n",
    "    pub function: Function<DI, DO>,\n",
    "    pub input_metric: MI,\n",
    "    pub output_metric: MO,\n",
    "    pub stability_relation: StabilityRelation<MI, MO>,\n",
    "}\n",
    "```\n",
    "\n",
    "Lets explain each of the struct members.\n",
    "```rust\n",
    "    ...\n",
    "    pub input_domain: DI,\n",
    "    pub output_domain: DO,\n",
    "    ...\n",
    "```\n",
    "The input and output domain strictly defines permissible input and output values.\n",
    "When you attempt to chain any two transformations, the output domain of the first transformation must match the input domain of the second transformation.\n",
    "The resulting chained transformation contains the input domain of the first transformation, the output domain of the second transformation, as well as the two functions composed.\n",
    "\n",
    "```rust\n",
    "    ...\n",
    "    pub function: Function<DI, DO>,\n",
    "    ...\n",
    "```\n",
    "When we invoked the following transformation, the python data structure was translated into a low-level C representation, the rust `function` was evaluated, and the result shipped back out to familiar python data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 456]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast_str_int([\"null\", \"1.\", \"2\", \"456\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have input and output metrics.\n",
    "```rust\n",
    "    ...\n",
    "    pub input_metric: MI,\n",
    "    pub output_metric: MO,\n",
    "    ...\n",
    "```\n",
    "Examples of metrics are `HammingDistance`, `SymmetricDistance`, `AbsoluteDistance` and `L1Distance`. \n",
    "They behave in the same way that the input and output domains do when chaining.\n",
    "\n",
    "```rust\n",
    "    ...\n",
    "    pub stability_relation: StabilityRelation<MI, MO>,\n",
    "    ...\n",
    "```\n",
    "Finally, the stability relation. \n",
    "It is a function that takes in an input and output distance, in the respective metric spaces, and returns a boolean.\n",
    "The function relates the input and output distances, returning False if the output distance is too small with respect to the input distance.\n",
    "\n",
    "For example, we know that the casting transformation is row-by-row, so we should expect that for any symmetric distance `a`, the pair of distances (`a`, `a`) are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "cast_str_int.check(d_in=a, d_out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast_str_int.check(d_in=a, d_out=a - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any two compatible transformations are chained, the resulting transformation contains a functional composition of the relations.\n",
    "\n",
    "Ultimately, all pieces are used to construct the new transformation:\n",
    "\n",
    "| input | chaining | output |\n",
    "|---:|:---:|:---|\n",
    "| input_domain_1 | output_domain_1 == input_domain_2 | output_domain_2 |\n",
    "| function_1 |composed with| function_2 |\n",
    "| input_metric_1 | output_metric_1 == input_metric_2 | output_metric_2 |\n",
    "| stability_relation_1 | composed with | stability_relation_2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen above, when we want to create a transformation, we use \"constructor\" functions. These are, by convention, prefixed with `make_`.\n",
    "\n",
    "An example implementation of a casting transformation constructor is provided. I'll break it down into three parts.\n",
    "\n",
    "```rust\n",
    "// 1.\n",
    "pub fn make_cast_default<DIA, TOA>() -> Fallible<Transformation<\n",
    "    VectorDomain<AllDomain<TIA>>, VectorDomain<AllDomain<TOA>>, \n",
    "    SymmetricDistance, SymmetricDistance>>\n",
    "\n",
    "    // 2.\n",
    "    where TIA: 'static + Clone + CheckNull, \n",
    "          TOA: 'static + RoundCast<TIA> + Default + CheckNull {\n",
    "\n",
    "    // 3.\n",
    "    Ok(Transformation::new(\n",
    "        VectorDomain::new(AllDomain::new()),\n",
    "        VectorDomain::new(AllDomain::new()),\n",
    "        Function::new(move |arg: &Vec<TIA>|\n",
    "            arg.iter().map(|v| TOA::round_cast(v.clone()).unwrap_or_default()).collect()),\n",
    "        SymmetricDistance::new(),\n",
    "        SymmetricDistance::new(),\n",
    "        StabilityRelation::new_from_constant(1)))\n",
    "}\n",
    "```\n",
    "\n",
    "The first part is the function signature:\n",
    "```rust\n",
    "pub fn make_cast_default<TIA, TOA>() -> Fallible<Transformation<\n",
    "    VectorDomain<AllDomain<TIA>>, VectorDomain<AllDomain<TOA>>, \n",
    "    SymmetricDistance, SymmetricDistance>>\n",
    "    ...\n",
    "```\n",
    "Most of the signature consists of types. \n",
    "Rust is strictly typed, so the code needs to be very explicit about what the type of the constructor function's inputs and outputs are. \n",
    "\n",
    "This is a generic function with two type arguments `TIA` and `TOA`, standing for \"atomic input type\" and \"atomic output type\".\n",
    "There are zero first-class arguments `()`.\n",
    "\n",
    "The constructor returns a fallible transformation.\n",
    "The last two lines specify the types of the input/output domains/metrics.\n",
    "\n",
    "The second part is the where clause:\n",
    "```rust\n",
    "    ...\n",
    "    where TIA: 'static + Clone + CheckNull, \n",
    "          TOA: 'static + RoundCast<TIA> + Default + CheckNull {\n",
    "    ...\n",
    "```\n",
    "A where clause lists bounds on the acceptable types to be used in the function.\n",
    "You can interpret this as, \"the compiler will enforce that `TIA` must be some type that has the `Clone` and `CheckNull` traits. \n",
    "In other words, while I don't specify what `TIA` must be up-front, I can bound what type it may be to types that are cloneable and have some concept of null-checking.\n",
    "`TOA`, in particular, has a `RoundCast` trait, which can be used to cast from type `TIA` to `TOA`. \n",
    "\n",
    "The final part is the function body, which just creates and implicitly returns a Transformation struct.\n",
    "```rust\n",
    "    ...\n",
    "    Ok(Transformation::new(\n",
    "        VectorDomain::new(AllDomain::new()),\n",
    "        VectorDomain::new(AllDomain::new()),\n",
    "        Function::new(move |arg: &Vec<TIA>|\n",
    "            arg.iter().map(|v| TOA::round_cast(v.clone()).unwrap_or_default()).collect()),\n",
    "        SymmetricDistance::new(),\n",
    "        SymmetricDistance::new(),\n",
    "        StabilityRelation::new_from_constant(1)))\n",
    "}\n",
    "```\n",
    "Each argument corresponds to a struct member.\n",
    "We take advantage of a handy syntax for creating un-named functions:\n",
    "In the example function addition function, `|a, b| a + b`. takes two arguments, `a` and `b`. The function body is `a + b`.\n",
    "\n",
    "With this shorthand in-hand, we create a function that casts the data by iterating over each record `v`, casting, and replacing nulls with the default value for the type.\n",
    "\n",
    "We also take advantage of a convenient constructor for building `c`-stable relations.\n",
    "Since the cast function is row-by-row, it is 1-stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Private Count\n",
    "Time to compute our first aggregate statistic.\n",
    "Suppose we want to know the number of records in the dataset.\n",
    "\n",
    "We can use the [list of aggregators](https://docs.opendp.org/en/stable/user/transformation-constructors.html#aggregators)\n",
    "in the Transformation Constructors section of the user guide to find `make_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from opendp.trans import make_count\n",
    "count = income_preprocessor >> make_count(TIA=int)\n",
    "# NOT a DP release!\n",
    "count_response = count(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful!\n",
    "`count` is still only a transformation,\n",
    "so the output in `count_response` is not a differentially private release.\n",
    "You will need to chain with a measurement to create a differentially private release.\n",
    "\n",
    "We use `make_base_geometric` below, because `make_base_geometric` has an integer support.\n",
    "Notice that we now import from `opendp.meas`, and the resulting `type(dp_count)` is `Measurement`. This tells us that the output will be a differentially private release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from opendp.meas import make_base_geometric\n",
    "dp_count = count >> make_base_geometric(scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In any realistic situation, you would likely want to estimate the budget utilization before you make a release.\n",
    "Use a search utility to quantify the privacy expenditure of this release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP count budget: 1.0\n",
      "DP count: 10001\n"
     ]
    }
   ],
   "source": [
    "from opendp.mod import binary_search\n",
    "\n",
    "# estimate the budget...\n",
    "epsilon = binary_search(\n",
    "    lambda eps: dp_count.check(d_in=max_influence, d_out=eps),\n",
    "    bounds=(0., 100.))\n",
    "print(\"DP count budget:\", epsilon)\n",
    "\n",
    "# ...and then release\n",
    "count_release = dp_count(data)\n",
    "print(\"DP count:\", count_release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Structure\n",
    "\n",
    "Measurements are very similar to Transformations, with two key differences.\n",
    "\n",
    "```rust\n",
    "pub struct Measurement<DI: Domain, DO: Domain, MI: Metric, MO: Measure> {\n",
    "    pub input_domain: DI,\n",
    "    pub output_domain: DO,\n",
    "    pub function: Function<DI, DO>,\n",
    "    pub input_metric: MI,\n",
    "    pub output_measure: MO,\n",
    "    pub privacy_relation: PrivacyRelation<MI, MO>,\n",
    "}\n",
    "```\n",
    "\n",
    "First, the `output_metric` is replaced with an `output_measure`, as distances in the output space are measured in terms of divergences between probability distributions.\n",
    "\n",
    "Second, the name of the relation has changed from a stability relation to a privacy relation. \n",
    "This is because the relation between distances now carries meaning with respect to privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Sum\n",
    "\n",
    "Suppose we want to know the total income of our dataset.\n",
    "First, take a look at [the list of aggregators](https://docs.opendp.org/en/stable/user/transformation-constructors.html#aggregators).\n",
    "`make_bounded_sum` seems to meet our requirements.\n",
    "As indicated by the function's API documentation, it expects bounded data,\n",
    "so we'll also need to chain the transformation from `make_clamp` with the `income_preprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from opendp.trans import make_clamp, make_bounded_sum\n",
    "bounded_income_sum = (\n",
    "    income_preprocessor >>\n",
    "    # Clamp income values\n",
    "    make_clamp(bounds=income_bounds) >>\n",
    "    # These bounds must be identical to the clamp bounds, otherwise chaining will fail\n",
    "    make_bounded_sum(bounds=income_bounds)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example, instead of just passing a scale into `make_base_geometric`,\n",
    "lets say I want whatever scale will make my measurement 1-epsilon DP.\n",
    "Again, I can use a search utility to find such a scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from opendp.mod import binary_search_param\n",
    "\n",
    "discovered_scale = binary_search_param(\n",
    "    lambda s: bounded_income_sum >> make_base_geometric(scale=s),\n",
    "    d_in=max_influence,\n",
    "    d_out=1.)\n",
    "\n",
    "dp_sum = bounded_income_sum >> make_base_geometric(scale=discovered_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more succinctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP sum: 1208520\n"
     ]
    }
   ],
   "source": [
    "from opendp.mod import binary_search_chain\n",
    "\n",
    "dp_sum = binary_search_chain(\n",
    "    lambda s: bounded_income_sum >> make_base_geometric(scale=s),\n",
    "    d_in=max_influence,\n",
    "    d_out=1.)\n",
    "\n",
    "# ...and make our 1-epsilon DP release\n",
    "print(\"DP sum:\", dp_sum(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Private Mean\n",
    "\n",
    "We may be more interested in the mean age.\n",
    "Just like before, we reference the docs to find `make_sized_bounded_mean`.\n",
    "The name of the constructor indicates that it expects sized, bounded data,\n",
    "and the docstring points us toward preprocessors we can use.\n",
    "\n",
    "Sized data is data that has a known number of rows.\n",
    "The constructor enforces this requirement\n",
    "because knowledge of the dataset size was necessary to establish the privacy proof.\n",
    "\n",
    "Luckily, we've already made a DP release of the number of rows in the dataset,\n",
    "which we can reuse as an argument here.\n",
    "\n",
    "Putting the previous sections together, our bounded mean age is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_split_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9279fe83c74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m mean_age_preprocessor = (\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Convert data into a dataframe of string columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmake_split_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Selects a column of df, Vec<str>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmake_select_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_split_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "from opendp.trans import make_cast_default, make_bounded_resize, make_sized_bounded_mean\n",
    "from opendp.mod import OpenDPException\n",
    "\n",
    "try:\n",
    "    mean_age_preprocessor = (\n",
    "        # Convert data into a dataframe of string columns\n",
    "        make_split_dataframe(separator=\",\", col_names=col_names) >>\n",
    "        # Selects a column of df, Vec<str>\n",
    "        make_select_column(key=\"age\", TOA=str) >>\n",
    "        # Cast the column as Vec<float>, and fill nulls with the default value, 0.\n",
    "        make_cast_default(TIA=str, TOA=float) >>\n",
    "        # Clamp age values\n",
    "        make_clamp(bounds=age_bounds)\n",
    "    )\n",
    "except OpenDPException as err:\n",
    "    assert err.message.startswith(\"Intermediate domains don't match.\")\n",
    "    print(err.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wait a second! The intermediate domains don't match?\n",
    "In this case, we casted to float-valued data, but `make_clamp` was built with integer-valued bounds,\n",
    "so the clamp is expecting integer data.\n",
    "Therefore, the output of the cast is not a valid input to the clamp.\n",
    "We can fix this by adjusting the bounds and trying again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "float_age_bounds = tuple(map(float, age_bounds))\n",
    "\n",
    "mean_age_preprocessor = (\n",
    "    # Convert data into a dataframe of string columns\n",
    "    make_split_dataframe(separator=\",\", col_names=col_names) >>\n",
    "    # Selects a column of df, Vec<str>\n",
    "    make_select_column(key=\"age\", TOA=str) >>\n",
    "    # Cast the column as Vec<float>, and fill nulls with the default value, 0.\n",
    "    make_cast_default(TIA=str, TOA=float) >>\n",
    "    # Clamp age values\n",
    "    make_clamp(bounds=float_age_bounds) >>\n",
    "    # Resize the dataset to length `count_release`.\n",
    "    #     If there are fewer than `count_release` rows in the data, fill with a constant of 20.\n",
    "    #     If there are more than `count_release` rows in the data, only keep `count_release` rows\n",
    "    make_bounded_resize(size=count_release, bounds=float_age_bounds, constant=20.) >>\n",
    "    # Compute the mean\n",
    "    make_sized_bounded_mean(size=count_release, bounds=float_age_bounds)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I stopped just short of chaining with a measurement because we're working with float data.\n",
    "There are some extra considerations to take in mind with floating-point data,\n",
    "which are [covered in the user guide](https://docs.opendp.org/en/latest/user/measurement-constructors.html#floating-point).\n",
    "\n",
    "With the assumption that you understand the ramifications, I'll go ahead and finish the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP mean: 0.21497149083820533\n"
     ]
    }
   ],
   "source": [
    "from opendp.meas import make_base_laplace\n",
    "enable_features(\"floating-point\")\n",
    "# add laplace noise\n",
    "dp_mean = mean_age_preprocessor >> make_base_laplace(scale=1.0)\n",
    "\n",
    "mean_release = dp_mean(data)\n",
    "print(\"DP mean:\", mean_release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Depending on your use-case, you may find greater utility separately releasing a DP sum and a DP count,\n",
    "and then postprocessing them into the mean.\n",
    "In the above mean example, you could even take advantage of this to avoid using floating-point numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Private Variance\n",
    "\n",
    "This is the last quick example, just to show a complete computation chain.\n",
    "In this example, I chain with the gaussian mechanism instead, with a budget of .1 epsilon, 1e-5 delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP variance: -1.039669609948424\n"
     ]
    }
   ],
   "source": [
    "from opendp.meas import make_base_gaussian\n",
    "from opendp.trans import make_sized_bounded_variance\n",
    "\n",
    "variance = (\n",
    "    # Convert data into a dataframe of string columns\n",
    "    make_split_dataframe(separator=\",\", col_names=col_names) >>\n",
    "    # Selects a column of df, Vec<str>\n",
    "    make_select_column(key=\"age\", TOA=str) >>\n",
    "    # Cast the column as Vec<float>, and fill nulls with the default value, 0.\n",
    "    make_cast_default(TIA=str, TOA=float) >>\n",
    "    # Clamp age values\n",
    "    make_clamp(bounds=float_age_bounds) >>\n",
    "    # Resize the dataset to length `count_release`.\n",
    "    make_bounded_resize(size=count_release, bounds=float_age_bounds, constant=20.) >>\n",
    "    # Compute the variance\n",
    "    make_sized_bounded_variance(size=count_release, bounds=float_age_bounds)\n",
    ")\n",
    "\n",
    "dp_variance = binary_search_chain(\n",
    "    lambda s: variance >> make_base_gaussian(scale=s),\n",
    "    d_in=max_influence,\n",
    "    d_out=(1., 1e-5) # (epsilon, delta)-DP\n",
    ")\n",
    "\n",
    "print(\"DP variance:\", dp_variance(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinators\n",
    "\n",
    "Combinators constructors that take other measurements or transformations as arguments.\n",
    "\n",
    "You're already familiar with chainers, which are a type of combinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 10, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendp.comb import make_chain_tt\n",
    "\n",
    "chained = make_chain_tt(clamper, caster)\n",
    "\n",
    "chained([\"1\", \"2\", \"3\", \"10\", \"a\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another kind of combinator is composition. We can combine multiple measurements to create one measurement that represents the composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(546144, 0.2937861188251779)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendp.comb import make_basic_composition\n",
    "\n",
    "composed = make_basic_composition(dp_sum, dp_mean)\n",
    "\n",
    "composed(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of combinator is an amplifier. In this example I'll apply the amplifier to a dp variance estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opendp.comb import make_population_amplification\n",
    "variance = make_sized_bounded_variance(size=count_release, bounds=float_age_bounds)\n",
    "\n",
    "dp_variance = binary_search_chain(\n",
    "    lambda s: variance >> make_base_laplace(scale=s),\n",
    "    d_in=max_influence,\n",
    "    d_out=1.\n",
    ")\n",
    "\n",
    "make_population_amplification(dp_variance, 100_000).check(1, .2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we found a dp variance estimator that was 1 epsilon-DP, but after amplification, it is at least .05 epsilon-DP. We are taking advantage of the knowledge that the dataset was a simple sample from a larger population with at least 100,000 individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information... \n",
    "- Docs website: https://docs.opendp.org\n",
    "- Github Repo:  https://github.com/opendp/opendp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrib, Vetting and Proofs\n",
    "\n",
    "As mentioned before, much of the library is still in \"contrib\".\n",
    "A requirement of the vetting process is having the code supported by a proof document. \n",
    "The library is designed to make this as easy as possible, because it consists of modular building blocks (Transformations and Measurements) for which encapsulated proofs may be written.\n",
    "\n",
    "Each Transformation or Measurement proof must show the following:\n",
    "1. That the function, when evaluated on any element in the input domain, emits a value in the output domain.\n",
    "2. That the relation always returns false if the function is not (d_in, d_out)-close for all d_in and d_out.\n",
    "3. That your choices of metrics/measures are compatible with your domains.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3220da548452ac41acb293d0d6efded0f046fab635503eb911c05f743e930f34"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('psi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63803d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendp.prelude as dp\n",
    "dp.enable_features(\"contrib\", \"rust-stack-trace\")\n",
    "import numpy as np\n",
    "from mbi import LinearMeasurement, Domain as MBIDomain, estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf7922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterating\n",
      "Privacy used so far: rho = 0.0062499999999999995\n",
      "iterating\n",
      "Privacy used so far: rho = 0.03125\n",
      "iterating\n",
      "Privacy used so far: rho = 0.13125\n",
      "iterating\n",
      "Privacy used so far: rho = 0.23125\n",
      "iterating\n",
      "Privacy used so far: rho = 0.33125000000000004\n",
      "iterating\n",
      "Privacy budget nearly exhausted (up to rounding error). Exiting the loop.\n",
      "Total privacy budget expended: rho = 0.42125000000000007\n",
      "Query [0, 1, 4] with weight 0.7: L1 distance per bin = 0.0007\n",
      "Query [2, 3, 4] with weight 0.7: L1 distance per bin = 0.0003\n",
      "Query [0, 1, 2] with weight 0.3: L1 distance per bin = 0.0005\n",
      "Query [1, 2] with weight 0.2: L1 distance per bin = 0.0009\n",
      "Query [1, 3] with weight 0.1: L1 distance per bin = 0.0016\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_rows = 10000\n",
    "data = np.zeros((n_rows, 5), dtype=int)\n",
    "\n",
    "cardinalities = [3, 4, 5, 7, 11]\n",
    "\n",
    "data[:, 0] = np.random.randint(0, cardinalities[0], size=n_rows)  # Values 0-2\n",
    "data[:, 1] = np.random.randint(0, cardinalities[1], size=n_rows)  # Values 0-3\n",
    "data[:, 2] = np.random.randint(0, cardinalities[2], size=n_rows)  # Values 0-4\n",
    "data[:, 3] = np.random.randint(0, cardinalities[3], size=n_rows)  # Values 0-6\n",
    "data[:, 4] = np.random.randint(0, cardinalities[4], size=n_rows)  # Values 0-10\n",
    "\n",
    "queries = [\n",
    "        [0, 1, 4],\n",
    "        [2, 3, 4],\n",
    "        [0, 1, 2],\n",
    "        [1, 2],\n",
    "        [1, 3]\n",
    "    ]\n",
    "weights = [0.7, 0.7, 0.3, 0.2, 0.1]\n",
    "\n",
    "releases = [\n",
    "    LinearMeasurement(\n",
    "        noisy_measurement=np.bincount(data[:, i], minlength=cardinalities[i]),\n",
    "        clique=(i,),\n",
    "    )\n",
    "    for i in range(len(cardinalities))\n",
    "]\n",
    "\n",
    "m_aim = dp.synthetic.make_ordinal_aim(\n",
    "    dp.numpy.array2_domain(cardinalities=cardinalities, T=int),\n",
    "    dp.symmetric_distance(),\n",
    "    dp.zero_concentrated_divergence(),\n",
    "    d_in=1,\n",
    "    d_out=0.5,\n",
    "    releases=releases,\n",
    "    queries=queries,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "noisy_releases = m_aim(data)\n",
    "\n",
    "mbi_domain = MBIDomain(\n",
    "    attributes=tuple(range(len(cardinalities))),\n",
    "    shape=cardinalities\n",
    ")\n",
    "\n",
    "model = estimation.mirror_descent(\n",
    "    mbi_domain,\n",
    "    noisy_releases,\n",
    "    iters=100,\n",
    "    callback_fn=lambda *_: None\n",
    ")\n",
    "\n",
    "synthetic_data = model.synthetic_data(10000)\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# print(type(synthetic_data))\n",
    "# print(dir(synthetic_data))\n",
    "\n",
    "synthetic_df = synthetic_data.df\n",
    "original_df = pd.DataFrame(data, columns=range(data.shape[1]))\n",
    "\n",
    "# print(\"Synthetic Data Sample:\")\n",
    "# print(synthetic_df.head())\n",
    "\n",
    "# print(\"Original Data Sample:\")\n",
    "# print(original_df.head())\n",
    "weights = [0.7, 0.7, 0.3, 0.2, 0.1]\n",
    "for query, weight in zip(queries, weights):\n",
    "    true_counts = original_df.groupby(query).size().div(len(original_df)) # gets the count divided by rows (i.e. probability) for each combo of things in the query columns (groupby)\n",
    "    aim_counts = synthetic_df.groupby(query).size().div(len(synthetic_df))\n",
    "    \n",
    "    all_keys = list(product(*[range(cardinalities[i]) for i in query]))\n",
    "    orig = true_counts.reindex(all_keys, fill_value=0) # reindex according to all possible cardinalities, fill the others w/ 0s so doesn't affect error\n",
    "    synth = aim_counts.reindex(all_keys, fill_value=0)\n",
    "\n",
    "    l1_distance = (orig - synth).abs().sum()\n",
    "    l1_per_bin = l1_distance / np.prod([cardinalities[i] for i in query])\n",
    "    print(f\"Query {query} with weight {weight}: L1 distance per bin = {l1_per_bin:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# created class to put this all together; just need queries + weights + cardinalities\n",
    "\n",
    "class AIMEval:\n",
    "    def __init__(self, queries, weights, cardinalities, d_in=1.0, d_out=0.5, iters=100):\n",
    "        assert len(queries) == len(weights)\n",
    "        self.queries = queries\n",
    "        self.weights = weights\n",
    "        self.cardinalities = cardinalities\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.iters = iters\n",
    "\n",
    "    def make_data(self, num_rows):\n",
    "        data = np.zeros((num_rows, len(self.cardinalities)), dtype=int)\n",
    "        for col in range(len(self.cardinalities)):\n",
    "            data[:, col] = np.random.randint(0, self.cardinalities[col], size=num_rows)\n",
    "        return data\n",
    "\n",
    "    def make_releases(self, data):\n",
    "        releases = [\n",
    "            LinearMeasurement(\n",
    "                noisy_measurement=np.bincount(data[:, i], minlength=self.cardinalities[i]),\n",
    "                clique=(i,)\n",
    "            )\n",
    "            for i in range(len(self.cardinalities))\n",
    "        ]\n",
    "        return releases\n",
    "\n",
    "    def make_noisy_releases(self, data, releases):\n",
    "        m_aim = dp.synthetic.make_ordinal_aim(\n",
    "            dp.numpy.array2_domain(cardinalities=self.cardinalities, T=int),\n",
    "            dp.symmetric_distance(),\n",
    "            dp.zero_concentrated_divergence(),\n",
    "            d_in=self.d_in,\n",
    "            d_out=self.d_out,\n",
    "            releases=releases,\n",
    "            queries=self.queries,\n",
    "            weights=self.weights\n",
    "        )\n",
    "        return m_aim(data)\n",
    "\n",
    "    def make_synthetic_data(self, noisy_releases, num_rows):\n",
    "        domain = MBIDomain(\n",
    "            attributes=tuple(range(len(self.cardinalities))),\n",
    "            shape=self.cardinalities\n",
    "        )\n",
    "        model = estimation.mirror_descent(\n",
    "            domain,\n",
    "            noisy_releases,\n",
    "            iters=self.iters,\n",
    "            callback_fn=lambda *_: None\n",
    "        )\n",
    "        return model.synthetic_data(num_rows)\n",
    "\n",
    "    def get_error_per_query(self, synth_df, true_df):\n",
    "        num_rows_true = len(true_df)\n",
    "        num_rows_synth = len(synth_df)\n",
    "        l1_distances_per_bin = {}\n",
    "\n",
    "        for query, weight in zip(self.queries, self.weights):\n",
    "            true_counts = true_df.groupby(query).size().div(num_rows_true)\n",
    "            synth_counts = synth_df.groupby(query).size().div(num_rows_synth)\n",
    "\n",
    "            all_keys = list(product(*[range(self.cardinalities[i]) for i in query]))\n",
    "            true_counts = true_counts.reindex(all_keys, fill_value=0)\n",
    "            synth_counts = synth_counts.reindex(all_keys, fill_value=0)\n",
    "\n",
    "            l1_distance = (true_counts - synth_counts).abs().sum()\n",
    "            l1_per_bin = l1_distance / np.prod([self.cardinalities[i] for i in query])\n",
    "            print(f\"Query {query} with weight {weight}: L1 distance per bin = {l1_per_bin:.4f}\")\n",
    "            l1_distances_per_bin[tuple(query)] = l1_per_bin\n",
    "\n",
    "        return l1_distances_per_bin\n",
    "    \n",
    "    def evaluate(self, num_rows):\n",
    "        data = self.make_data(num_rows)\n",
    "        releases = self.make_releases(data)\n",
    "        noisy_releases = self.make_noisy_releases(data, releases)\n",
    "        synth_data = self.make_synthetic_data(noisy_releases, num_rows)\n",
    "\n",
    "        synth_df = synth_data.df\n",
    "        true_df = data.df\n",
    "        errors = self.get_error_per_query(synth_df, true_df)\n",
    "        return synth_df, errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries + weights (None) + cardinalities\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "# one query with all columns \n",
    "# each col queried separately\n",
    "# random combos\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
